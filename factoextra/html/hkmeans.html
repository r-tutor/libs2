<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: Hierarchical k-means clustering</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body>

<table width="100%" summary="page for hkmeans {factoextra}"><tr><td>hkmeans {factoextra}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>Hierarchical k-means clustering</h2>

<h3>Description</h3>

<p>The final k-means clustering solution is very sensitive to the initial random selection 
of cluster centers. This function provides a solution using an hybrid approach by combining 
the hierarchical clustering and the k-means methods. The procedure is explained in &quot;Details&quot; section. Read more:
<a href="http://www.sthda.com/english/wiki/hybrid-hierarchical-k-means-clustering-for-optimizing-clustering-outputs-unsupervised-machine-learning">Hybrid hierarchical k-means clustering for optimizing clustering outputs</a>.
</p>

<ul>
<li><p> hkmeans(): compute hierarchical k-means clustering
</p>
</li>
<li><p> print.hkmeans(): prints the result of hkmeans
</p>
</li>
<li><p> hkmeans_tree(): plots the initial dendrogram
</p>
</li></ul>



<h3>Usage</h3>

<pre>
hkmeans(x, k, hc.metric = "euclidean", hc.method = "ward.D2",
  iter.max = 10, km.algorithm = "Hartigan-Wong")

## S3 method for class 'hkmeans'
print(x, ...)

hkmeans_tree(hkmeans, rect.col = NULL, ...)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>x</code></td>
<td>
<p>a numeric matrix, data frame or vector</p>
</td></tr>
<tr valign="top"><td><code>k</code></td>
<td>
<p>the number of clusters to be generated</p>
</td></tr>
<tr valign="top"><td><code>hc.metric</code></td>
<td>
<p>the distance measure to be used. Possible values are &quot;euclidean&quot;, &quot;maximum&quot;, &quot;manhattan&quot;, 
&quot;canberra&quot;, &quot;binary&quot; or &quot;minkowski&quot; (see ?dist).</p>
</td></tr>
<tr valign="top"><td><code>hc.method</code></td>
<td>
<p>the agglomeration method to be used. Possible values include &quot;ward.D&quot;, &quot;ward.D2&quot;, &quot;single&quot;, 
&quot;complete&quot;, &quot;average&quot;, &quot;mcquitty&quot;, &quot;median&quot;or &quot;centroid&quot; (see ?hclust).</p>
</td></tr>
<tr valign="top"><td><code>iter.max</code></td>
<td>
<p>the maximum number of iterations allowed for k-means.</p>
</td></tr>
<tr valign="top"><td><code>km.algorithm</code></td>
<td>
<p>the algorithm to be used for kmeans (see ?kmeans).</p>
</td></tr>
<tr valign="top"><td><code>...</code></td>
<td>
<p>others arguments to be passed to the function plot.hclust(); (see ? plot.hclust)</p>
</td></tr>
<tr valign="top"><td><code>hkmeans</code></td>
<td>
<p>an object of class hkmeans (returned by the function hkmeans())</p>
</td></tr>
<tr valign="top"><td><code>rect.col</code></td>
<td>
<p>Vector with border colors for the rectangles around clusters in dendrogram</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The procedure is as follow:   
</p>
<p>1. Compute hierarchical clustering
</p>
<p>2. Cut the tree in k-clusters
</p>
<p>3. compute the center (i.e the mean) of each cluster
</p>
<p>4. Do k-means by using the set of cluster centers (defined in step 3) as the initial cluster centers. Optimize the clustering.  
</p>
<p>This means that the final optimized partitioning obtained at step 4 might be different from the initial partitioning obtained at step 2. 
Consider mainly the result displayed by <code>fviz_cluster()</code>.
</p>


<h3>Value</h3>

<p>hkmeans returns an object of class &quot;hkmeans&quot; containing the following components:
</p>

<ul>
<li><p> The elements returned by the standard function kmeans() (see ?kmeans)
</p>
</li>
<li><p> data: the data used for the analysis
</p>
</li>
<li><p> hclust: an object of class &quot;hclust&quot; generated by the function hclust()
</p>
</li></ul>



<h3>Examples</h3>

<pre>

# Load data
data(USArrests)
# Scale the data
df &lt;- scale(USArrests)

# Compute hierarchical k-means clustering
res.hk &lt;-hkmeans(df, 4)

# Elements returned by hkmeans()
names(res.hk)

# Print the results
res.hk

# Visualize the tree
hkmeans_tree(res.hk, cex = 0.6)
# or use this
fviz_dend(res.hk, cex = 0.6)


# Visualize the hkmeans final clusters
fviz_cluster(res.hk, frame.type = "norm", frame.level = 0.68)

</pre>

<hr /><div style="text-align: center;">[Package <em>factoextra</em> version 1.0.6 <a href="00Index.html">Index</a>]</div>
</body></html>
