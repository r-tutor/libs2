<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: Convex Clustering</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body>

<table width="100%" summary="page for cclust {flexclust}"><tr><td>cclust {flexclust}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>Convex Clustering</h2>

<h3>Description</h3>

<p>Perform k-means clustering, hard competitive learning or neural gas on
a data matrix.
</p>


<h3>Usage</h3>

<pre>
cclust(x, k, dist = "euclidean", method = "kmeans",
       weights=NULL, control=NULL, group=NULL, simple=FALSE,
       save.data=FALSE)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>x</code></td>
<td>
<p>A numeric matrix of data, or an object that can be coerced to
such a matrix (such as a numeric vector or a data frame with all
numeric columns).</p>
</td></tr>
<tr valign="top"><td><code>k</code></td>
<td>
<p>Either the number of clusters, or a vector of cluster
assignments, or a matrix of initial
(distinct) cluster centroids.  If a number, a random set of (distinct)
rows in <code>x</code> is chosen as the initial centroids.</p>
</td></tr>
<tr valign="top"><td><code>dist</code></td>
<td>
<p>Distance measure, one of <code>"euclidean"</code> (mean square
distance) or <code>"manhattan "</code> (absolute distance).</p>
</td></tr>
<tr valign="top"><td><code>method</code></td>
<td>
<p>Clustering algorithm: one of <code>"kmeans"</code>,
<code>"hardcl"</code> or <code>"neuralgas"</code>, see details below.</p>
</td></tr>
<tr valign="top"><td><code>weights</code></td>
<td>
<p>An optional vector of weights to be used in the fitting
process. Works only in combination with hard competitive learning.</p>
</td></tr>
<tr valign="top"><td><code>control</code></td>
<td>
<p>An object of class <code>cclustControl</code>.</p>
</td></tr>
<tr valign="top"><td><code>group</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
<tr valign="top"><td><code>simple</code></td>
<td>
<p>Return an object of class <code>kccasimple</code>?</p>
</td></tr>
<tr valign="top"><td><code>save.data</code></td>
<td>
<p>Save a copy of <code>x</code> in the return object?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses the same computational engine as the earlier
function of the same name from package &lsquo;cclust&rsquo;. The main difference
is that it returns an S4 object of class <code>"kcca"</code>, hence all
available methods for <code>"kcca"</code> objects can be used. By default
<code><a href="kcca.html">kcca</a></code> and <code>cclust</code> use exactly the same algorithm,
but <code>cclust</code> will usually be much faster because it uses compiled
code.
</p>
<p>If <code>dist</code> is <code>"euclidean"</code>, the distance between the cluster
center and the data points is the Euclidian distance (ordinary kmeans
algorithm), and cluster means are used as centroids.
If <code>"manhattan"</code>, the distance between the cluster
center and the data points is the sum of the absolute values of the
distances, and the column-wise cluster medians are used as centroids.
</p>
<p>If <code>method</code> is <code>"kmeans"</code>, the classic kmeans algorithm as
given by MacQueen (1967) is
used, which works by repeatedly moving all cluster
centers to the mean of their respective Voronoi sets. If
<code>"hardcl"</code>,
on-line updates are used (AKA hard competitive learning), which work by
randomly drawing an observation from <code>x</code> and moving the closest
center towards that point (e.g., Ripley 1996). If
<code>"neuralgas"</code> then the  neural gas algorithm by Martinetz et al
(1993) is used. It is similar to hard competitive learning, but in
addition to the closest centroid also the second closest centroid is
moved in each iteration.
</p>


<h3>Value</h3>

<p>An object of class <code>"kcca"</code>.
</p>


<h3>Author(s)</h3>

<p>Evgenia Dimitriadou and Friedrich Leisch</p>


<h3>References</h3>

<p>MacQueen, J. (1967).  Some methods for classification and analysis of
multivariate observations. In <em>Proceedings of the Fifth Berkeley
Symposium on  Mathematical Statistics and  Probability</em>,
eds L. M. Le Cam \&amp; J. Neyman, 1, pp. 281&ndash;297. Berkeley, CA:
University of California Press.
</p>
<p>Martinetz T., Berkovich S., and Schulten K (1993). &lsquo;Neural-Gas&rsquo;
Network for Vector Quantization and its Application to Time-Series
Prediction. IEEE Transactions on Neural Networks, 4 (4), pp. 558&ndash;569.
</p>
<p>Ripley, B. D. (1996)
<em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>


<h3>See Also</h3>

<p><code><a href="flexclustControl-class.html">cclustControl-class</a></code>, <code><a href="kcca.html">kcca</a></code>
</p>


<h3>Examples</h3>

<pre>
## a 2-dimensional example
x&lt;-rbind(matrix(rnorm(100,sd=0.3),ncol=2),
         matrix(rnorm(100,mean=1,sd=0.3),ncol=2))
cl&lt;-cclust(x,2)
plot(x, col=predict(cl))
points(cl@centers, pch="x", cex=2, col=3) 

## a 3-dimensional example 
x&lt;-rbind(matrix(rnorm(150,sd=0.3),ncol=3),
         matrix(rnorm(150,mean=2,sd=0.3),ncol=3),
         matrix(rnorm(150,mean=4,sd=0.3),ncol=3))
cl&lt;-cclust(x, 6, method="neuralgas", save.data=TRUE)
pairs(x, col=predict(cl))
plot(cl)
</pre>

<hr /><div style="text-align: center;">[Package <em>flexclust</em> version 1.3-5 <a href="00Index.html">Index</a>]</div>
</body></html>
